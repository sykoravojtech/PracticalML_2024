PROJECT_NAME: PracticalML
EXPERIMENT: cls_vm_ch_exp7
OUTPUT_DIR: ./runs
WANDB_KEY: 547d969d74fa2db0a0eb6a8b49490f5fddccfc04
SEED: 42

MODEL:
  TYPE: classification
  ENCODER:
    TYPE: VideoMamba
    PRETRAINED: True
    MODEL_SIZE: middle # tiny, small, middle
    HIDDEN_SIZE: 576   # 192,  384,   576
    RETURN_ALL_HIDDEN: False
    NUM_VISUAL_TOKENS: 1 # Number of visual tokens' hidden states to use in the head. 
                            # If RETURN_ALL_HIDDEN is False, this is ignored.

  HEAD:
    TYPE: MLP
    REDUCER_TYPE: token_wise # Options: token_wise, hidden_wise. If RETURN_ALL_HIDDEN is False, this is ignored.
    LAYERS: [1024, 1024]
    DROPOUT: 0.7  # Optional: Dropout probability
    LAYER_NORM: False  # Optional: Use layer normalization
    NUM_CLASSES: 157
    MULTI_LABEL: True
    THRESHOLD: 0.5

  LOSS: BCEWithLogitsLoss
  USE_CLASS_WEIGHTS: True # 2

DATA:
  ENCODING_DIR: data/encodings/cls_vm_charades/ # path to precomputed hidden states from the encoder
  DATASET: charades_action_classification
  ROOT_PATH: data/raw/Charades
  TRAIN_CSV: Charades_per-frame_annotations_fps1.5_action_cls_train.csv
  TEST_CSV: Charades_per-frame_annotations_fps1.5_action_cls_test.csv
  IMG_SIZE: 224
  NUM_SAMPLED_FRAMES: 16
  CLIP_DURATION: 11 # we want about 16 frames, fps = 1.5 -> set clip duration to 11s
  FPS: 1.5
  MEAN: [0.485, 0.456, 0.406] # [0.45, 0.45, 0.45] 
  STD: [0.229, 0.224, 0.225] #  [0.225, 0.225, 0.225]
  NUM_WORKERS: 16

TRAIN:
  FREEZE_ENCODER: True
  NUM_EPOCHS: 300
  BATCH_SIZE: 32
  OPTIM:
    TYPE: AdamW
    INIT_LEARNING_RATE: 0.0001
    MIN_LEARNING_RATE: 0.000001
    EPS: 0.000001
    BETAS: [0.9, 0.999]
    LR_MILESTONES: [250,400]
  LOG_STEPS: 50
  ACCELERATOR: auto
  DEVICES: auto
  PRECISION: 32
  BEST_CHECKPOINT_BY: val_mAP
  COMPUTE_METRIC_AT_TRAIN_TIME: True
